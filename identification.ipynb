{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "def chunk_search(segment, chunked):\n",
    "    m = len(chunked)\n",
    "    list1 = []\n",
    "    for j in range(m):\n",
    "        if (len(chunked[j]) > 2 or len(chunked[j]) == 1):\n",
    "            list1.append(j)\n",
    "        if (len(chunked[j]) == 2):\n",
    "            try:\n",
    "                str1 = chunked[j][0][0] + \" \" + chunked[j][1][0]\n",
    "            except Exception:\n",
    "                pass\n",
    "            else:\n",
    "                if (str1 in segment) == True:\n",
    "                    list1.append(j)\n",
    "    return list1\n",
    "\n",
    "def segment_identify(sen):\n",
    "    segment_set = sen.split(\",\")\n",
    "    return segment_set\n",
    "\n",
    "\n",
    "def clause_identify(segment):\n",
    "    tok = nltk.word_tokenize(segment)\n",
    "    tag = nltk.pos_tag(tok)\n",
    "    gram = r\"\"\"chunk:{<EX>?<DT>?<JJ.?>*<NN.?|PRP|PRP\\$|POS|IN|DT|CC|VBG|VBN>+<RB.?|VB.?|MD|RP>+}\"\"\"\n",
    "    chunkparser = nltk.RegexpParser(gram)\n",
    "    chunked = chunkparser.parse(tag)\n",
    "\n",
    "    flag = 0\n",
    "    for j in range(len(chunked)):\n",
    "        if (len(chunked[j]) > 2):\n",
    "            flag = 1\n",
    "        if (len(chunked[j]) == 2):\n",
    "            try:\n",
    "                str1 = chunked[j][0][0] + \" \" + chunked[j][1][0]\n",
    "            except Exception:\n",
    "                pass\n",
    "            else:\n",
    "                if (str1 in segment) == True:\n",
    "                    flag = 1\n",
    "        if flag == 1:\n",
    "            break\n",
    "\n",
    "    return flag\n",
    "\n",
    "\n",
    "def verbphrase_identify(clause):\n",
    "    tok = nltk.word_tokenize(clause)\n",
    "    tag = nltk.pos_tag(tok)\n",
    "    gram = r\"\"\"chunk:{<EX>?<DT>?<JJ.?>*<NN.?|PRP|PRP\\$|POS|IN|DT|CC|VBG|VBN>+<RB.?>*<VB.?|MD|RP>+}\"\"\"\n",
    "    chunkparser = nltk.RegexpParser(gram)\n",
    "    chunked = chunkparser.parse(tag)\n",
    "    str1 = \"\"\n",
    "    str2 = \"\"\n",
    "    str3 = \"\"\n",
    "    list1 = chunk_search(clause, chunked)\n",
    "    if len(list1) != 0:\n",
    "        m = list1[len(list1) - 1]\n",
    "        for j in range(len(chunked[m])):\n",
    "            str1 += chunked[m][j][0]\n",
    "            str1 += \" \"\n",
    "\n",
    "    tok1 = nltk.word_tokenize(str1)\n",
    "    tag1 = nltk.pos_tag(tok1)\n",
    "    gram1 = r\"\"\"chunk:{<EX>?<DT>?<JJ.?>*<NN.?|PRP|PRP\\$|POS|IN|DT|CC|VBG|VBN>+<RB.?>*}\"\"\"\n",
    "    chunkparser1 = nltk.RegexpParser(gram1)\n",
    "    chunked1 = chunkparser1.parse(tag1)\n",
    "\n",
    "    list2 = chunk_search(str1, chunked1)\n",
    "    if len(list2) != 0:\n",
    "\n",
    "        m = list2[0]\n",
    "        for j in range(len(chunked1[m])):\n",
    "            str2 += (chunked1[m][j][0] + \" \")\n",
    "\n",
    "    tok1 = nltk.word_tokenize(str1)\n",
    "    tag1 = nltk.pos_tag(tok1)\n",
    "    gram1 = r\"\"\"chunk:{<VB.?|MD|RP>+}\"\"\"\n",
    "    chunkparser1 = nltk.RegexpParser(gram1)\n",
    "    chunked2 = chunkparser1.parse(tag1)\n",
    "\n",
    "    list3 = chunk_search(str1, chunked2)\n",
    "    if len(list3) != 0:\n",
    "\n",
    "        m = list3[0]\n",
    "        for j in range(len(chunked2[m])):\n",
    "            str3 += (chunked2[m][j][0] + \" \")\n",
    "\n",
    "    X = \"\"\n",
    "    str4 = \"\"\n",
    "    st = nltk.word_tokenize(str3)\n",
    "    if len(st) > 1:\n",
    "        X = st[0]\n",
    "        s = \"\"\n",
    "        for k in range(1, len(st)):\n",
    "            s += st[k]\n",
    "            s += \" \"\n",
    "        str3 = s\n",
    "        str4 = X + \" \" + str2 + str3\n",
    "\n",
    "    if len(st) == 1:\n",
    "        tag1 = nltk.pos_tag(st)\n",
    "        if tag1[0][0] != 'are' and tag1[0][0] != 'were' and tag1[0][0] != 'is' and tag1[0][0] != 'am':\n",
    "            if tag1[0][1] == 'VB' or tag1[0][1] == 'VBP':\n",
    "                X = 'do'\n",
    "            if tag1[0][1] == 'VBD' or tag1[0][1] == 'VBN':\n",
    "                X = 'did'\n",
    "            if tag1[0][1] == 'VBZ':\n",
    "                X = 'does'\n",
    "            str4 = X + \" \" + str2 + str3\n",
    "        if (tag1[0][0] == 'are' or tag1[0][0] == 'were' or tag1[0][0] == 'is' or tag1[0][0] == 'am'):\n",
    "            str4 = tag1[0][0] + \" \" + str2\n",
    "\n",
    "    return str4\n",
    "\n",
    "\n",
    "def subjectphrase_search(segment_set, num):\n",
    "    str2 = \"\"\n",
    "    for j in range(num - 1, 0, -1):\n",
    "        str1 = \"\"\n",
    "        flag = 0\n",
    "        tok = nltk.word_tokenize(segment_set[j])\n",
    "        tag = nltk.pos_tag(tok)\n",
    "        gram = r\"\"\"chunk:{<EX>?<DT>?<JJ.?>*<NN.?|PRP|PRP\\$|POS|IN|DT|CC|VBG|VBN>+<RB.?>*<VB.?|MD|RP>+}\"\"\"\n",
    "        chunkparser = nltk.RegexpParser(gram)\n",
    "        chunked = chunkparser.parse(tag)\n",
    "\n",
    "        list1 = chunk_search(segment_set[j], chunked)\n",
    "        if len(list1) != 0:\n",
    "            m = list1[len(list1) - 1]\n",
    "            for j in range(len(chunked[m])):\n",
    "                str1 += chunked[m][j][0]\n",
    "                str1 += \" \"\n",
    "\n",
    "            tok1 = nltk.word_tokenize(str1)\n",
    "            tag1 = nltk.pos_tag(tok1)\n",
    "            gram1 = r\"\"\"chunk:{<EX>?<DT>?<JJ.?>*<NN.?|PRP|PRP\\$|POS|IN|DT|CC|VBG|VBN>+}\"\"\"\n",
    "            chunkparser1 = nltk.RegexpParser(gram1)\n",
    "            chunked1 = chunkparser1.parse(tag1)\n",
    "\n",
    "            list2 = chunk_search(str1, chunked1)\n",
    "            if len(list2) != 0:\n",
    "                m = list2[len(list2) - 1]\n",
    "                for j in range(len(chunked1[m])):\n",
    "                    str2 += (chunked1[m][j][0] + \" \")\n",
    "                flag = 1\n",
    "\n",
    "        if flag == 0:\n",
    "            tok1 = nltk.word_tokenize(segment_set[j])\n",
    "            tag1 = nltk.pos_tag(tok1)\n",
    "            gram1 = r\"\"\"chunk:{<EX>?<DT>?<JJ.?>*<NN.?|PRP|PRP\\$|POS|IN|DT|CC|VBG|VBN>+}\"\"\"\n",
    "            chunkparser1 = nltk.RegexpParser(gram1)\n",
    "            chunked1 = chunkparser1.parse(tag1)\n",
    "\n",
    "            list2 = chunk_search(str1, chunked1)\n",
    "            st = nltk.word_tokenize(segment_set[j])\n",
    "            if len(chunked1[list2[0]]) == len(st):\n",
    "                str2 = segment_set[j]\n",
    "                flag = 1\n",
    "\n",
    "        if flag == 1:\n",
    "            break\n",
    "\n",
    "    return str2\n",
    "\n",
    "\n",
    "def postprocess(string):\n",
    "    tok = nltk.word_tokenize(string)\n",
    "    tag = nltk.pos_tag(tok)\n",
    "\n",
    "    str1 = tok[0].capitalize()\n",
    "    str1 += \" \"\n",
    "    if len(tok) != 0:\n",
    "        for i in range(1, len(tok)):\n",
    "            if tag[i][1] == \"NNP\":\n",
    "                str1 += tok[i].capitalize()\n",
    "                str1 += \" \"\n",
    "            else:\n",
    "                str1 += tok[i].lower()\n",
    "                str1 += \" \"\n",
    "        tok = nltk.word_tokenize(str1)\n",
    "        str1 = \"\"\n",
    "        for i in range(len(tok)):\n",
    "            if tok[i] == \"i\" or tok[i] == \"we\":\n",
    "                str1 += \"you\"\n",
    "                str1 += \" \"\n",
    "            elif tok[i] == \"my\" or tok[i] == \"our\":\n",
    "                str1 += \"your\"\n",
    "                str1 += \" \"\n",
    "            elif tok[i] == \"your\":\n",
    "                str1 += \"my\"\n",
    "                str1 += \" \"\n",
    "            elif tok[i] == \"you\":\n",
    "                if i - 1 >= 0:\n",
    "                    to = nltk.word_tokenize(tok[i - 1])\n",
    "                    ta = nltk.pos_tag(to)\n",
    "                    # print ta\n",
    "                    if ta[0][1] == 'IN':\n",
    "                        str1 += \"me\"\n",
    "                        str1 += \" \"\n",
    "                    else:\n",
    "                        str1 += \"i\"\n",
    "                        str1 += \" \"\n",
    "                else:\n",
    "                    str1 += \"i \"\n",
    "\n",
    "            elif tok[i] == \"am\":\n",
    "                str1 += \"are\"\n",
    "                str1 += \" \"\n",
    "            else:\n",
    "                str1 += tok[i]\n",
    "                str1 += \" \"\n",
    "\n",
    "    return str1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
