{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import identification\n",
    "\n",
    "\n",
    "def get_chunk(chunked):\n",
    "    str1 = \"\"\n",
    "    for j in range(len(chunked)):\n",
    "        str1 += (chunked[j][0] + \" \")\n",
    "    return str1\n",
    "\n",
    "def what_whom1(segment_set, num, ner):\n",
    "    tok = nltk.word_tokenize(segment_set[num])\n",
    "    tag = nltk.pos_tag(tok)\n",
    "    gram = r\"\"\"chunk:{<TO>+<DT>?<RB.?>*<JJ.?>*<NN.?|PRP|PRP\\$|VBG|DT|POS|CD|VBN>+}\"\"\"\n",
    "    chunkparser = nltk.RegexpParser(gram)\n",
    "    chunked = chunkparser.parse(tag)\n",
    "\n",
    "    list1 = identification.chunk_search(segment_set[num], chunked)\n",
    "    s = []\n",
    "\n",
    "    if len(list1) != 0:\n",
    "        for j in range(len(chunked)):\n",
    "            str1 = \"\"\n",
    "            str3 = \"\"\n",
    "            if j in list1:\n",
    "                for k in range(j):\n",
    "                    if k in list1:\n",
    "                        str1 += get_chunk(chunked[k])\n",
    "                    else:\n",
    "                        str1 += (chunked[k][0] + \" \")\n",
    "                for k in range(j + 1, len(chunked)):\n",
    "                    if k in list1:\n",
    "                        str3 += get_chunk(chunked[k])\n",
    "                    else:\n",
    "                        str3 += (chunked[k][0] + \" \")\n",
    "\n",
    "                if chunked[j][1][1] == 'PRP':\n",
    "                    str2 = \"to whom \"\n",
    "                else:\n",
    "                    for x in range(len(chunked[j])):\n",
    "                        if (chunked[j][x][1] == \"NNP\" or chunked[j][x][1] == \"NNPS\" or chunked[j][x][1] == \"NNS\" or\n",
    "                                chunked[j][x][1] == \"NN\"):\n",
    "                            break\n",
    "\n",
    "                    for x1 in range(len(ner)):\n",
    "                        if ner[x1][0] == chunked[j][x][0]:\n",
    "                            if ner[x1][1] == \"PERSON\":\n",
    "                                str2 = \" to whom \"\n",
    "                            elif ner[x1][1] == \"LOC\" or ner[x1][1] == \"ORG\" or ner[x1][1] == \"GPE\":\n",
    "                                str2 = \" where \"\n",
    "                            elif ner[x1][1] == \"TIME\" or ner[x1][1] == \"DATE\":\n",
    "                                str2 = \" when \"\n",
    "                            else:\n",
    "                                str2 = \"to what\"\n",
    "\n",
    "                str4 = str1 + str2 + str3\n",
    "                for k in range(len(segment_set)):\n",
    "                    if k != num:\n",
    "                        str4 += (\",\" + segment_set[k])\n",
    "                str4 += '?'\n",
    "                str4 = identification.postprocess(str4)\n",
    "                # str4 = 'Q.' + str4\n",
    "                s.append(str4)\n",
    "    return s\n",
    "\n",
    "\n",
    "def what_whom2(segment_set, num, ner):\n",
    "    tok = nltk.word_tokenize(segment_set[num])\n",
    "    tag = nltk.pos_tag(tok)\n",
    "    gram = r\"\"\"chunk:{<IN>+<DT>?<RB.?>*<JJ.?>*<NN.?|PRP|PRP\\$|POS|VBG|DT|CD|VBN>+}\"\"\"\n",
    "    chunkparser = nltk.RegexpParser(gram)\n",
    "    chunked = chunkparser.parse(tag)\n",
    "    list1 = identification.chunk_search(segment_set[num], chunked)\n",
    "    s = []\n",
    "\n",
    "    if len(list1) != 0:\n",
    "        for j in range(len(chunked)):\n",
    "            str1 = \"\"\n",
    "            str3 = \"\"\n",
    "            if j in list1:\n",
    "                for k in range(j):\n",
    "                    if k in list1:\n",
    "                        str1 += get_chunk(chunked[k])\n",
    "                    else:\n",
    "                        str1 += (chunked[k][0] + \" \")\n",
    "                for k in range(j + 1, len(chunked)):\n",
    "                    if k in list1:\n",
    "                        str3 += get_chunk(chunked[k])\n",
    "                    else:\n",
    "                        str3 += (chunked[k][0] + \" \")\n",
    "\n",
    "                if chunked[j][1][1] == 'PRP':\n",
    "                    str2 = \" \" + chunked[j][0][0] + \" whom \"\n",
    "                else:\n",
    "                    for x in range(len(chunked[j])):\n",
    "                        if (chunked[j][x][1] == \"NNP\" or chunked[j][x][1] == \"NNPS\" or chunked[j][x][1] == \"NNS\" or\n",
    "                                chunked[j][x][1] == \"NN\"):\n",
    "                            break\n",
    "\n",
    "                    for x1 in range(len(ner)):\n",
    "                        if ner[x1][0] == chunked[j][x][0]:\n",
    "                            if ner[x1][1] == \"PERSON\":\n",
    "                                str2 = \" \" + chunked[j][0][0] + \"whom \"\n",
    "                            elif ner[x1][1] == \"LOC\" or ner[x1][1] == \"ORG\" or ner[x1][1] == \"GPE\":\n",
    "                                str2 = \" where \"\n",
    "                            elif ner[x1][1] == \"TIME\" or ner[x1][1] == \"DATE\":\n",
    "                                str2 = \" when \"\n",
    "                            else:\n",
    "                                str2 = \" \" + chunked[j][0][0] + \" what\"\n",
    "\n",
    "                str4 = str1 + str2 + str3\n",
    "                for k in range(len(segment_set)):\n",
    "                    if k != num:\n",
    "                        str4 += (\",\" + segment_set[k])\n",
    "                str4 += '?'\n",
    "                str4 = identification.postprocess(str4)\n",
    "                # str4 = 'Q.' + str4\n",
    "                s.append(str4)\n",
    "    return s\n",
    "\n",
    "\n",
    "def whose(segment_set, num, ner):\n",
    "    tok = nltk.word_tokenize(segment_set[num])\n",
    "    tag = nltk.pos_tag(tok)\n",
    "    gram = r\"\"\"chunk:{<NN.?>*<PRP\\$|POS>+<RB.?>*<JJ.?>*<NN.?|VBG|VBN>+}\"\"\"\n",
    "    chunkparser = nltk.RegexpParser(gram)\n",
    "    chunked = chunkparser.parse(tag)\n",
    "\n",
    "    list1 = identification.chunk_search(segment_set[num], chunked)\n",
    "    s = []\n",
    "\n",
    "    if len(list1) != 0:\n",
    "        for j in range(len(chunked)):\n",
    "            str1 = \"\"\n",
    "            str3 = \"\"\n",
    "            str2 = \" whose \"\n",
    "            if j in list1:\n",
    "                for k in range(j):\n",
    "                    if k in list1:\n",
    "                        str1 += get_chunk(chunked[k])\n",
    "                    else:\n",
    "                        str1 += (chunked[k][0] + \" \")\n",
    "                for k in range(j + 1, len(chunked)):\n",
    "                    if k in list1:\n",
    "                        str3 += get_chunk(chunked[k])\n",
    "                    else:\n",
    "                        str3 += (chunked[k][0] + \" \")\n",
    "                if chunked[j][1][1] == 'POS':\n",
    "                    for k in range(2, len(chunked[j])):\n",
    "                        str2 += (chunked[j][k][0] + \" \")\n",
    "                else:\n",
    "                    for k in range(1, len(chunked[j])):\n",
    "                        str2 += (chunked[j][k][0] + \" \")\n",
    "\n",
    "                str4 = str1 + str2 + str3\n",
    "                for k in range(len(segment_set)):\n",
    "                    if k != num:\n",
    "                        str4 += (\",\" + segment_set[k])\n",
    "                str4 += '?'\n",
    "                str4 = identification.postprocess(str4)\n",
    "                # str4 = 'Q.' + str4\n",
    "                s.append(str4)\n",
    "    return s\n",
    "\n",
    "\n",
    "def howmany(segment_set, num, ner):\n",
    "    tok = nltk.word_tokenize(segment_set[num])\n",
    "    tag = nltk.pos_tag(tok)\n",
    "    gram = r\"\"\"chunk:{<DT>?<CD>+<RB>?<JJ|JJR|JJS>?<NN|NNS|NNP|NNPS|VBG>+}\"\"\"\n",
    "    chunkparser = nltk.RegexpParser(gram)\n",
    "    chunked = chunkparser.parse(tag)\n",
    "\n",
    "    list1 = identification.chunk_search(segment_set[num], chunked)\n",
    "    s = []\n",
    "\n",
    "    if len(list1) != 0:\n",
    "        for j in range(len(chunked)):\n",
    "            str1 = \"\"\n",
    "            str3 = \"\"\n",
    "            str2 = \" how many \"\n",
    "            if j in list1:\n",
    "                for k in range(j):\n",
    "                    if k in list1:\n",
    "                        str1 += get_chunk(chunked[k])\n",
    "                    else:\n",
    "                        str1 += (chunked[k][0] + \" \")\n",
    "                for k in range(j + 1, len(chunked)):\n",
    "                    if k in list1:\n",
    "                        str3 += get_chunk(chunked[k])\n",
    "                    else:\n",
    "                        str3 += (chunked[k][0] + \" \")\n",
    "\n",
    "                st = get_chunk(chunked[j])\n",
    "                tok = nltk.word_tokenize(st)\n",
    "                tag = nltk.pos_tag(tok)\n",
    "                gram = r\"\"\"chunk:{<RB>?<JJ|JJR|JJS>?<NN|NNS|NNP|NNPS|VBG>+}\"\"\"\n",
    "                chunkparser = nltk.RegexpParser(gram)\n",
    "                chunked1 = chunkparser.parse(tag)\n",
    "\n",
    "                list2 = identification.chunk_search(st, chunked1)\n",
    "                z = \"\"\n",
    "\n",
    "                for k in range(len(chunked1)):\n",
    "                    if k in list2:\n",
    "                        z += get_chunk(chunked1[k])\n",
    "\n",
    "                str4 = str1 + str2 + z + str3\n",
    "                for k in range(len(segment_set)):\n",
    "                    if k != num:\n",
    "                        str4 += (\",\" + segment_set[k])\n",
    "                str4 += '?'\n",
    "                str4 = identification.postprocess(str4)\n",
    "                # str4 = 'Q.' + str4\n",
    "                s.append(str4)\n",
    "    return s\n",
    "\n",
    "\n",
    "def howmuch_1(segment_set, num, ner):\n",
    "    tok = nltk.word_tokenize(segment_set[num])\n",
    "    tag = nltk.pos_tag(tok)\n",
    "    gram = r\"\"\"chunk:{<IN>+<\\$>?<CD>+}\"\"\"\n",
    "    chunkparser = nltk.RegexpParser(gram)\n",
    "    chunked = chunkparser.parse(tag)\n",
    "\n",
    "    list1 = identification.chunk_search(segment_set[num], chunked)\n",
    "    s = []\n",
    "\n",
    "    if len(list1) != 0:\n",
    "        for j in range(len(chunked)):\n",
    "            str1 = \"\"\n",
    "            str3 = \"\"\n",
    "            str2 = \" how much \"\n",
    "            if j in list1:\n",
    "                for k in range(j):\n",
    "                    if k in list1:\n",
    "                        str1 += get_chunk(chunked[k])\n",
    "                    else:\n",
    "                        str1 += (chunked[k][0] + \" \")\n",
    "                for k in range(j + 1, len(chunked)):\n",
    "                    if k in list1:\n",
    "                        str3 += get_chunk(chunked[k])\n",
    "                    else:\n",
    "                        str3 += (chunked[k][0] + \" \")\n",
    "\n",
    "                str2 = chunked[j][0][0] + str2\n",
    "                str4 = str1 + str2 + str3\n",
    "                for k in range(len(segment_set)):\n",
    "                    if k != num:\n",
    "                        str4 += (\",\" + segment_set[k])\n",
    "                str4 += '?'\n",
    "                str4 = identification.postprocess(str4)\n",
    "                # str4 = 'Q.' + str4\n",
    "                s.append(str4)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
