{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import identification\n",
    "import nonClause\n",
    "\n",
    "\n",
    "def whom_1(segment_set, num, ner):\n",
    "    tok = nltk.word_tokenize(segment_set[num])\n",
    "    tag = nltk.pos_tag(tok)\n",
    "    gram = r\"\"\"chunk:{<TO>+<DT>?<RB.?>*<JJ.?>*<NN.?|PRP|PRP\\$|VBG|DT|POS|CD|VBN>+}\"\"\"\n",
    "    chunkparser = nltk.RegexpParser(gram)\n",
    "    chunked = chunkparser.parse(tag)\n",
    "\n",
    "    list1 = identification.chunk_search(segment_set[num], chunked)\n",
    "    list3 = []\n",
    "\n",
    "    if len(list1) != 0:\n",
    "        for j in range(len(chunked)):\n",
    "            str1 = \"\"\n",
    "            str2 = \"\"\n",
    "            str3 = \"\"\n",
    "            if j in list1:\n",
    "                for k in range(j):\n",
    "                    if k in list1:\n",
    "                        str1 += nonClause.get_chunk(chunked[k])\n",
    "                    else:\n",
    "                        str1 += (chunked[k][0] + \" \")\n",
    "\n",
    "                for k in range(j + 1, len(chunked)):\n",
    "                    if k in list1:\n",
    "                        str3 += nonClause.get_chunk(chunked[k])\n",
    "                    else:\n",
    "                        str3 += (chunked[k][0] + \" \")\n",
    "\n",
    "                if chunked[j][1][1] == 'PRP':\n",
    "                    str2 = \" to whom \"\n",
    "                else:\n",
    "                    for x in range(len(chunked[j])):\n",
    "                        if (chunked[j][x][1] == \"NNP\" or chunked[j][x][1] == \"NNPS\" or chunked[j][x][1] == \"NNS\" or\n",
    "                                chunked[j][x][1] == \"NN\"):\n",
    "                            break\n",
    "\n",
    "                    for x1 in range(len(ner)):\n",
    "\n",
    "                        if ner[x1][0] == chunked[j][x][0]:\n",
    "                            if ner[x1][1] == \"PERSON\":\n",
    "                                str2 = \" to whom \"\n",
    "                            elif ner[x1][1] == \"LOC\" or ner[x1][1] == \"ORG\" or ner[x1][1] == \"GPE\":\n",
    "                                str2 = \" where \"\n",
    "                            elif ner[x1][1] == \"TIME\" or ner[x1][1] == \"DATE\":\n",
    "                                str2 = \" when \"\n",
    "                            else:\n",
    "                                str2 = \"to what \"\n",
    "\n",
    "                tok = nltk.word_tokenize(str1)\n",
    "                tag = nltk.pos_tag(tok)\n",
    "                gram = r\"\"\"chunk:{<EX>?<DT>?<JJ.?>*<NN.?|PRP|PRP\\$|POS|IN|DT|CC|VBG|VBN>+<RB.?>*<VB.?|MD|RP>+}\"\"\"\n",
    "                chunkparser = nltk.RegexpParser(gram)\n",
    "                chunked1 = chunkparser.parse(tag)\n",
    "\n",
    "                list2 = identification.chunk_search(str1, chunked1)\n",
    "                if len(list2) != 0:\n",
    "                    m = list2[len(list2) - 1]\n",
    "\n",
    "                    str4 = nonClause.get_chunk(chunked1[m])\n",
    "                    str4 = identification.verbphrase_identify(str4)\n",
    "                    str5 = \"\"\n",
    "                    str6 = \"\"\n",
    "\n",
    "                    for k in range(m):\n",
    "                        if k in list2:\n",
    "                            str5 += nonClause.get_chunk(chunked1[k])\n",
    "                        else:\n",
    "                            str5 += (chunked1[k][0] + \" \")\n",
    "\n",
    "                    for k in range(m + 1, len(chunked1)):\n",
    "                        if k in list2:\n",
    "                            str6 += nonClause.get_chunk(chunked1[k])\n",
    "                        else:\n",
    "                            str6 += (chunked1[k][0] + \" \")\n",
    "\n",
    "                    st = str5 + str2 + str4 + str6 + str3\n",
    "                    for l in range(num + 1, len(segment_set)):\n",
    "                        st += (\",\" + segment_set[l])\n",
    "                    st += '?'\n",
    "                    st = identification.postprocess(st)\n",
    "                    # st = 'Q.' + st\n",
    "                    list3.append(st)\n",
    "\n",
    "    return list3\n",
    "\n",
    "\n",
    "def whom_2(segment_set, num, ner):\n",
    "    tok = nltk.word_tokenize(segment_set[num])\n",
    "    tag = nltk.pos_tag(tok)\n",
    "    gram = r\"\"\"chunk:{<IN>+<DT>?<RB.?>*<JJ.?>*<NN.?|PRP|PRP\\$|POS|VBG|DT|CD|VBN>+}\"\"\"\n",
    "    chunkparser = nltk.RegexpParser(gram)\n",
    "    chunked = chunkparser.parse(tag)\n",
    "\n",
    "    list1 = identification.chunk_search(segment_set[num], chunked)\n",
    "    list3 = []\n",
    "\n",
    "    if len(list1) != 0:\n",
    "        for j in range(len(chunked)):\n",
    "            str1 = \"\"\n",
    "            str2 = \"\"\n",
    "            str3 = \"\"\n",
    "            if j in list1:\n",
    "                for k in range(j):\n",
    "                    if k in list1:\n",
    "                        str1 += nonClause.get_chunk(chunked[k])\n",
    "                    else:\n",
    "                        str1 += (chunked[k][0] + \" \")\n",
    "\n",
    "                for k in range(j + 1, len(chunked)):\n",
    "                    if k in list1:\n",
    "                        str3 += nonClause.get_chunk(chunked[k])\n",
    "                    else:\n",
    "                        str3 += (chunked[k][0] + \" \")\n",
    "\n",
    "                if chunked[j][1][1] == 'PRP':\n",
    "                    str2 = \" \" + chunked[j][0][0] + \" whom \"\n",
    "                else:\n",
    "                    for x in range(len(chunked[j])):\n",
    "                        if (chunked[j][x][1] == \"NNP\" or chunked[j][x][1] == \"NNPS\" or chunked[j][x][1] == \"NNS\" or\n",
    "                                chunked[j][x][1] == \"NN\"):\n",
    "                            break\n",
    "\n",
    "                    for x1 in range(len(ner)):\n",
    "                        if ner[x1][0] == chunked[j][x][0]:\n",
    "                            if ner[x1][1] == \"PERSON\":\n",
    "                                str2 = \" \" + chunked[j][0][0] + \" whom \"\n",
    "                            elif ner[x1][1] == \"LOC\" or ner[x1][1] == \"ORG\" or ner[x1][1] == \"GPE\":\n",
    "                                str2 = \" where \"\n",
    "                            elif ner[x1][1] == \"TIME\" or ner[x1][1] == \"DATE\":\n",
    "                                str2 = \" when \"\n",
    "                            else:\n",
    "                                str2 = \" \" + chunked[j][0][0] + \" what \"\n",
    "\n",
    "                tok = nltk.word_tokenize(str1)\n",
    "                tag = nltk.pos_tag(tok)\n",
    "                gram = r\"\"\"chunk:{<EX>?<DT>?<JJ.?>*<NN.?|PRP|PRP\\$|POS|IN|DT|CC|VBG|VBN>+<RB.?>*<VB.?|MD|RP>+}\"\"\"\n",
    "                chunkparser = nltk.RegexpParser(gram)\n",
    "                chunked1 = chunkparser.parse(tag)\n",
    "\n",
    "                list2 = identification.chunk_search(str1, chunked1)\n",
    "                if len(list2) != 0:\n",
    "                    m = list2[len(list2) - 1]\n",
    "\n",
    "                    str4 = nonClause.get_chunk(chunked1[m])\n",
    "                    str4 = identification.verbphrase_identify(str4)\n",
    "                    str5 = \"\"\n",
    "                    str6 = \"\"\n",
    "\n",
    "                    for k in range(m):\n",
    "                        if k in list2:\n",
    "                            str5 += nonClause.get_chunk(chunked1[k])\n",
    "                        else:\n",
    "                            str5 += (chunked1[k][0] + \" \")\n",
    "\n",
    "                    for k in range(m + 1, len(chunked1)):\n",
    "                        if k in list2:\n",
    "                            str6 += nonClause.get_chunk(chunked1[k])\n",
    "                        else:\n",
    "                            str6 += (chunked1[k][0] + \" \")\n",
    "\n",
    "                    st = str5 + str2 + str4 + str6 + str3\n",
    "                    for l in range(num + 1, len(segment_set)):\n",
    "                        st += (\",\" + segment_set[l])\n",
    "                    st += '?'\n",
    "                    st = identification.postprocess(st)\n",
    "                    # st = 'Q.' + st\n",
    "                    list3.append(st)\n",
    "\n",
    "    return list3\n",
    "\n",
    "\n",
    "def whom_3(segment_set, num, ner):\n",
    "    tok = nltk.word_tokenize(segment_set[num])\n",
    "    tag = nltk.pos_tag(tok)\n",
    "    gram = r\"\"\"chunk:{<VB.?|MD|RP>+<DT>?<RB.?>*<JJ.?>*<NN.?|PRP|PRP\\$|POS|VBG|DT|CD|VBN>+}\"\"\"\n",
    "    chunkparser = nltk.RegexpParser(gram)\n",
    "    chunked = chunkparser.parse(tag)\n",
    "\n",
    "    list1 = identification.chunk_search(segment_set[num], chunked)\n",
    "    list3 = []\n",
    "\n",
    "    if len(list1) != 0:\n",
    "        for j in range(len(chunked)):\n",
    "            str1 = \"\"\n",
    "            str2 = \"\"\n",
    "            str3 = \"\"\n",
    "            if j in list1:\n",
    "                for k in range(j):\n",
    "                    if k in list1:\n",
    "                        str1 += nonClause.get_chunk(chunked[k])\n",
    "                    else:\n",
    "                        str1 += (chunked[k][0] + \" \")\n",
    "\n",
    "                for k in range(j + 1, len(chunked)):\n",
    "                    if k in list1:\n",
    "                        str3 += nonClause.get_chunk(chunked[k])\n",
    "                    else:\n",
    "                        str3 += (chunked[k][0] + \" \")\n",
    "\n",
    "                if chunked[j][1][1] == 'PRP':\n",
    "                    str2 = \" whom \"\n",
    "                else:\n",
    "                    for x in range(len(chunked[j])):\n",
    "                        if (chunked[j][x][1] == \"NNP\" or chunked[j][x][1] == \"NNPS\" or chunked[j][x][1] == \"NNS\" or\n",
    "                                chunked[j][x][1] == \"NN\"):\n",
    "                            break\n",
    "\n",
    "                    for x1 in range(len(ner)):\n",
    "                        if ner[x1][0] == chunked[j][x][0]:\n",
    "                            if ner[x1][1] == \"PERSON\":\n",
    "                                str2 = \" whom \"\n",
    "                            elif ner[x1][1] == \"LOC\" or ner[x1][1] == \"ORG\" or ner[x1][1] == \"GPE\":\n",
    "                                str2 = \" what \"\n",
    "                            elif ner[x1][1] == \"TIME\" or ner[x1][1] == \"DATE\":\n",
    "                                str2 = \" what time \"\n",
    "                            else:\n",
    "                                str2 = \" what \"\n",
    "\n",
    "                strx = nonClause.get_chunk(chunked[j])\n",
    "                tok = nltk.word_tokenize(strx)\n",
    "                tag = nltk.pos_tag(tok)\n",
    "                gram = r\"\"\"chunk:{<VB.?|MD>+}\"\"\"\n",
    "                chunkparser = nltk.RegexpParser(gram)\n",
    "                chunked1 = chunkparser.parse(tag)\n",
    "\n",
    "                strx = nonClause.get_chunk(chunked1[0])\n",
    "\n",
    "                str1 += strx\n",
    "\n",
    "                tok = nltk.word_tokenize(str1)\n",
    "                tag = nltk.pos_tag(tok)\n",
    "                gram = r\"\"\"chunk:{<EX>?<DT>?<JJ.?>*<NN.?|PRP|PRP\\$|POS|IN|DT|CC|VBG|VBN>+<RB.?>*<VB.?|MD|RP>+}\"\"\"\n",
    "                chunkparser = nltk.RegexpParser(gram)\n",
    "                chunked1 = chunkparser.parse(tag)\n",
    "\n",
    "                list2 = identification.chunk_search(str1, chunked1)\n",
    "\n",
    "                if len(list2) != 0:\n",
    "                    m = list2[len(list2) - 1]\n",
    "\n",
    "                    str4 = nonClause.get_chunk(chunked1[m])\n",
    "                    str4 = identification.verbphrase_identify(str4)\n",
    "                    str5 = \"\"\n",
    "                    str6 = \"\"\n",
    "\n",
    "                    for k in range(m):\n",
    "                        if k in list2:\n",
    "                            str5 += nonClause.get_chunk(chunked1[k])\n",
    "                        else:\n",
    "                            str5 += (chunked1[k][0] + \" \")\n",
    "\n",
    "                    for k in range(m + 1, len(chunked1)):\n",
    "                        if k in list2:\n",
    "                            str6 += nonClause.get_chunk(chunked1[k])\n",
    "                        else:\n",
    "                            str6 += (chunked1[k][0] + \" \")\n",
    "\n",
    "                    st = str5 + str2 + str4 + str6 + str3\n",
    "                    for l in range(num + 1, len(segment_set)):\n",
    "                        st += (\",\" + segment_set[l])\n",
    "                    st += '?'\n",
    "                    st = identification.postprocess(st)\n",
    "                    # st = 'Q.' + st\n",
    "                    list3.append(st)\n",
    "\n",
    "    return list3\n",
    "\n",
    "\n",
    "def whose(segment_set, num, ner):\n",
    "    tok = nltk.word_tokenize(segment_set[num])\n",
    "    tag = nltk.pos_tag(tok)\n",
    "    gram = r\"\"\"chunk:{<DT|NN.?>*<PRP\\$|POS>+<RB.?>*<JJ.?>*<NN.?|VBG|VBN>+<RB.?>*<VB.?|MD|RP>+}\"\"\"\n",
    "    chunkparser = nltk.RegexpParser(gram)\n",
    "    chunked = chunkparser.parse(tag)\n",
    "\n",
    "    list1 = identification.chunk_search(segment_set[num], chunked)\n",
    "    list3 = []\n",
    "\n",
    "    if len(list1) != 0:\n",
    "        for i in range(len(chunked)):\n",
    "            if i in list1:\n",
    "                str1 = \"\"\n",
    "                str3 = \"\"\n",
    "                str2 = \"\"\n",
    "                for k in range(i):\n",
    "                    if k in list1:\n",
    "                        str1 += nonClause.get_chunk(chunked[k])\n",
    "                    else:\n",
    "                        str1 += (chunked[k][0] + \" \")\n",
    "                str1 += \" whose \"\n",
    "\n",
    "                for k in range(i + 1, len(chunked)):\n",
    "                    if k in list1:\n",
    "                        str3 += nonClause.get_chunk(chunked[k])\n",
    "                    else:\n",
    "                        str3 += (chunked[k][0] + \" \")\n",
    "\n",
    "                if chunked[i][1][1] == 'POS':\n",
    "                    for k in range(2, len(chunked[i])):\n",
    "                        str2 += (chunked[i][k][0] + \" \")\n",
    "\n",
    "                if chunked[i][0][1] == 'PRP$':\n",
    "                    for k in range(1, len(chunked[i])):\n",
    "                        str2 += (chunked[i][k][0] + \" \")\n",
    "\n",
    "                str2 = str1 + str2 + str3\n",
    "                str4 = \"\"\n",
    "\n",
    "                for l in range(0, len(segment_set)):\n",
    "                    if l < num:\n",
    "                        str4 += (segment_set[l] + \",\")\n",
    "                    if l > num:\n",
    "                        str2 += (\",\" + segment_set[l])\n",
    "                str2 = str4 + str2\n",
    "                str2 += '?'\n",
    "                str2 = identification.postprocess(str2)\n",
    "                # str2 = 'Q.' + str2\n",
    "                list3.append(str2)\n",
    "\n",
    "    return list3\n",
    "\n",
    "\n",
    "def what_to_do(segment_set, num, ner):\n",
    "    tok = nltk.word_tokenize(segment_set[num])\n",
    "    tag = nltk.pos_tag(tok)\n",
    "    gram = r\"\"\"chunk:{<TO>+<VB|VBP|RP>+<DT>?<RB.?>*<JJ.?>*<NN.?|PRP|PRP\\$|POS|VBG|DT>*}\"\"\"\n",
    "    chunkparser = nltk.RegexpParser(gram)\n",
    "    chunked = chunkparser.parse(tag)\n",
    "\n",
    "    list1 = identification.chunk_search(segment_set[num], chunked)\n",
    "    list3 = []\n",
    "\n",
    "    if len(list1) != 0:\n",
    "        for j in range(len(chunked)):\n",
    "            str1 = \"\"\n",
    "            str2 = \"\"\n",
    "            str3 = \"\"\n",
    "            if j in list1:\n",
    "                for k in range(j):\n",
    "                    if k in list1:\n",
    "                        str1 += nonClause.get_chunk(chunked[k])\n",
    "                    else:\n",
    "                        str1 += (chunked[k][0] + \" \")\n",
    "\n",
    "                for k in range(j + 1, len(chunked)):\n",
    "                    if k in list1:\n",
    "                        str3 += nonClause.get_chunk(chunked[k])\n",
    "                    else:\n",
    "                        str3 += (chunked[k][0] + \" \")\n",
    "\n",
    "                ls = nonClause.get_chunk(chunked[j])\n",
    "                tok = nltk.word_tokenize(ls)\n",
    "                tag = nltk.pos_tag(tok)\n",
    "                gram = r\"\"\"chunk:{<DT>?<RB.?>*<JJ.?>*<NN.?|PRP|PRP\\$|POS|VBG|DT>+}\"\"\"\n",
    "                chunkparser = nltk.RegexpParser(gram)\n",
    "                chunked2 = chunkparser.parse(tag)\n",
    "                lis = identification.chunk_search(ls, chunked2)\n",
    "                if len(lis) != 0:\n",
    "                    x = lis[len(lis) - 1]\n",
    "                    ls1 = nonClause.get_chunk(chunked2[x])\n",
    "                    index = ls.find(ls1)\n",
    "                    str2 = \" \" + ls[0:index]\n",
    "                else:\n",
    "                    str2 = \" to do \"\n",
    "\n",
    "                tok = nltk.word_tokenize(str1)\n",
    "                tag = nltk.pos_tag(tok)\n",
    "                gram = r\"\"\"chunk:{<EX>?<DT>?<JJ.?>*<NN.?|PRP|PRP\\$|POS|IN|DT|CC|VBG|VBN>+<RB.?>*<VB.?|MD|RP>+}\"\"\"\n",
    "                chunkparser = nltk.RegexpParser(gram)\n",
    "                chunked1 = chunkparser.parse(tag)\n",
    "\n",
    "                list2 = identification.chunk_search(str1, chunked1)\n",
    "                if len(list2) != 0:\n",
    "                    m = list2[len(list2) - 1]\n",
    "\n",
    "                    str4 = nonClause.get_chunk(chunked1[m])\n",
    "                    str4 = identification.verbphrase_identify(str4)\n",
    "                    str5 = \"\"\n",
    "                    str6 = \"\"\n",
    "\n",
    "                    for k in range(m):\n",
    "                        if k in list2:\n",
    "                            str5 += nonClause.get_chunk(chunked1[k])\n",
    "                        else:\n",
    "                            str5 += (chunked1[k][0] + \" \")\n",
    "\n",
    "                    for k in range(m + 1, len(chunked1)):\n",
    "                        if k in list2:\n",
    "                            str6 += nonClause.get_chunk(chunked1[k])\n",
    "                        else:\n",
    "                            str6 += (chunked1[k][0] + \" \")\n",
    "\n",
    "                    if chunked2[j][1][1] == 'PRP':\n",
    "                        tr = \" whom \"\n",
    "                    else:\n",
    "                        for x in range(len(chunked[j])):\n",
    "                            if (chunked[j][x][1] == \"NNP\" or chunked[j][x][1] == \"NNPS\" or chunked[j][x][1] == \"NNS\" or\n",
    "                                    chunked[j][x][1] == \"NN\"):\n",
    "                                break\n",
    "\n",
    "                        for x1 in range(len(ner)):\n",
    "                            if ner[x1][0] == chunked[j][x][0]:\n",
    "                                if ner[x1][1] == \"PERSON\":\n",
    "                                    tr = \" whom \"\n",
    "                                elif ner[x1][1] == \"LOC\" or ner[x1][1] == \"ORG\" or ner[x1][1] == \"GPE\":\n",
    "                                    tr = \" where \"\n",
    "                                elif ner[x1][1] == \"TIME\" or ner[x1][1] == \"DATE\":\n",
    "                                    tr = \" when \"\n",
    "                                else:\n",
    "                                    tr = \" what \"\n",
    "\n",
    "                    st = str5 + tr + str4 + str2 + str6 + str3\n",
    "                    for l in range(num + 1, len(segment_set)):\n",
    "                        st += (\",\" + segment_set[l])\n",
    "                    st += '?'\n",
    "                    st = identification.postprocess(st)\n",
    "                    # st = 'Q.' + st\n",
    "                    list3.append(st)\n",
    "\n",
    "    return list3\n",
    "\n",
    "\n",
    "def who(segment_set, num, ner):\n",
    "    tok = nltk.word_tokenize(segment_set[num])\n",
    "    tag = nltk.pos_tag(tok)\n",
    "    gram = r\"\"\"chunk:{<EX>?<DT>?<JJ.?>*<NN.?|PRP|PRP\\$|POS|IN|DT|CC|VBG|VBN>+<RB.?>*<VB.?|MD|RP>+}\"\"\"\n",
    "    chunkparser = nltk.RegexpParser(gram)\n",
    "    chunked = chunkparser.parse(tag)\n",
    "\n",
    "    list1 = identification.chunk_search(segment_set[num], chunked)\n",
    "    list3 = []\n",
    "\n",
    "    if len(list1) != 0:\n",
    "        for j in range(len(list1)):\n",
    "            m = list1[j]\n",
    "            str1 = \"\"\n",
    "            for k in range(m + 1, len(chunked)):\n",
    "                if k in list1:\n",
    "                    str1 += nonClause.get_chunk(chunked[k])\n",
    "                else:\n",
    "                    str1 += (chunked[k][0] + \" \")\n",
    "\n",
    "            str2 = nonClause.get_chunk(chunked[m])\n",
    "            tok = nltk.word_tokenize(str2)\n",
    "            tag = nltk.pos_tag(tok)\n",
    "\n",
    "            for m11 in range(len(tag)):\n",
    "                if tag[m11][1] == 'NNP' or tag[m11][1] == 'NNPS' or tag[m11][1] == 'NNS' or tag[m11][1] == 'NN':\n",
    "                    break\n",
    "            s11 = ' who '\n",
    "            for m12 in range(len(ner)):\n",
    "                if ner[m12][0] == tag[m11][0]:\n",
    "                    if ner[m12][1] == 'LOC':\n",
    "                        s11 = ' which place '\n",
    "                    elif ner[m12][1] == 'ORG':\n",
    "                        s11 = ' who '\n",
    "                    elif ner[m12][1] == 'DATE' or ner[m12][1] == 'TIME':\n",
    "                        s11 = ' what time '\n",
    "                    else:\n",
    "                        s11 = ' who '\n",
    "\n",
    "            gram = r\"\"\"chunk:{<RB.?>*<VB.?|MD|RP>+}\"\"\"\n",
    "            chunkparser = nltk.RegexpParser(gram)\n",
    "            chunked1 = chunkparser.parse(tag)\n",
    "\n",
    "            list2 = identification.chunk_search(str2, chunked1)\n",
    "            if len(list2) != 0:\n",
    "                str2 = nonClause.get_chunk(chunked1[list2[0]])\n",
    "                str2 = s11 + str2\n",
    "                for k in range(list2[0] + 1, len(chunked1)):\n",
    "                    if k in list2:\n",
    "                        str2 += nonClause.get_chunk(chunked[k])\n",
    "                    else:\n",
    "                        str2 += (chunked[k][0] + \" \")\n",
    "                str2 += (\" \" + str1)\n",
    "\n",
    "                tok_1 = nltk.word_tokenize(str2)\n",
    "                str2 = \"\"\n",
    "                for h in range(len(tok_1)):\n",
    "                    if tok_1[h] == \"am\":\n",
    "                        str2 += \" is \"\n",
    "                    else:\n",
    "                        str2 += (tok_1[h] + \" \")\n",
    "\n",
    "                for l in range(num + 1, len(segment_set)):\n",
    "                    str2 += (\",\" + segment_set[l])\n",
    "                str2 += '?'\n",
    "\n",
    "                str2 = identification.postprocess(str2)\n",
    "                # str2 = 'Q.' + str2\n",
    "                list3.append(str2)\n",
    "\n",
    "    return list3\n",
    "\n",
    "\n",
    "def howmuch_2(segment_set, num, ner):\n",
    "    tok = nltk.word_tokenize(segment_set[num])\n",
    "    tag = nltk.pos_tag(tok)\n",
    "    gram = r\"\"\"chunk:{<\\$>*<CD>+<MD>?<VB|VBD|VBG|VBP|VBN|VBZ|RP>+}\"\"\"\n",
    "    chunkparser = nltk.RegexpParser(gram)\n",
    "    chunked = chunkparser.parse(tag)\n",
    "\n",
    "    list1 = identification.chunk_search(segment_set[num], chunked)\n",
    "    list3 = []\n",
    "\n",
    "    if len(list1) != 0:\n",
    "        for j in range(len(list1)):\n",
    "            m = list1[j]\n",
    "            str1 = \"\"\n",
    "            for k in range(m + 1, len(chunked)):\n",
    "                if k in list1:\n",
    "                    str1 += nonClause.get_chunk(chunked[k])\n",
    "                else:\n",
    "                    str1 += (chunked[k][0] + \" \")\n",
    "\n",
    "            str2 = nonClause.get_chunk(chunked[m])\n",
    "            tok = nltk.word_tokenize(str2)\n",
    "            tag = nltk.pos_tag(tok)\n",
    "            gram = r\"\"\"chunk:{<RB.?>*<VB.?|MD|RP>+}\"\"\"\n",
    "            chunkparser = nltk.RegexpParser(gram)\n",
    "            chunked1 = chunkparser.parse(tag)\n",
    "            s11 = ' how much '\n",
    "\n",
    "            list2 = identification.chunk_search(str2, chunked1)\n",
    "            if len(list2) != 0:\n",
    "                str2 = nonClause.get_chunk(chunked1[list2[0]])\n",
    "                str2 = s11 + str2\n",
    "                for k in range(list2[0] + 1, len(chunked1)):\n",
    "                    if k in list2:\n",
    "                        str2 += nonClause.get_chunk(chunked[k])\n",
    "                    else:\n",
    "                        str2 += (chunked[k][0] + \" \")\n",
    "                str2 += (\" \" + str1)\n",
    "\n",
    "                tok_1 = nltk.word_tokenize(str2)\n",
    "                str2 = \"\"\n",
    "                for h in range(len(tok_1)):\n",
    "                    if tok_1[h] == \"am\":\n",
    "                        str2 += \" is \"\n",
    "                    else:\n",
    "                        str2 += (tok_1[h] + \" \")\n",
    "\n",
    "                for l in range(num + 1, len(segment_set)):\n",
    "                    str2 += (\",\" + segment_set[l])\n",
    "                str2 += '?'\n",
    "\n",
    "                str2 = identification.postprocess(str2)\n",
    "                # str2 = 'Q.' + str2\n",
    "                list3.append(str2)\n",
    "\n",
    "    return list3\n",
    "\n",
    "\n",
    "def howmuch_1(segment_set, num, ner):\n",
    "    tok = nltk.word_tokenize(segment_set[num])\n",
    "    tag = nltk.pos_tag(tok)\n",
    "    gram = r\"\"\"chunk:{<IN>+<\\$>?<CD>+}\"\"\"\n",
    "    chunkparser = nltk.RegexpParser(gram)\n",
    "    chunked = chunkparser.parse(tag)\n",
    "\n",
    "    list1 = identification.chunk_search(segment_set[num], chunked)\n",
    "    list3 = []\n",
    "\n",
    "    if len(list1) != 0:\n",
    "        for j in range(len(chunked)):\n",
    "            str1 = \"\"\n",
    "            str2 = \"\"\n",
    "            str3 = \"\"\n",
    "            if j in list1:\n",
    "                for k in range(j):\n",
    "                    if k in list1:\n",
    "                        str1 += nonClause.get_chunk(chunked[k])\n",
    "                    else:\n",
    "                        str1 += (chunked[k][0] + \" \")\n",
    "\n",
    "                for k in range(j + 1, len(chunked)):\n",
    "                    if k in list1:\n",
    "                        str3 += nonClause.get_chunk(chunked[k])\n",
    "                    else:\n",
    "                        str3 += (chunked[k][0] + \" \")\n",
    "\n",
    "                str2 = ' ' + chunked[j][0][0] + ' how much '\n",
    "\n",
    "                tok = nltk.word_tokenize(str1)\n",
    "                tag = nltk.pos_tag(tok)\n",
    "                gram = r\"\"\"chunk:{<EX>?<DT>?<JJ.?>*<NN.?|PRP|PRP\\$|POS|IN|DT|CC|VBG|VBN>+<RB.?>*<VB.?|MD|RP>+}\"\"\"\n",
    "                chunkparser = nltk.RegexpParser(gram)\n",
    "                chunked1 = chunkparser.parse(tag)\n",
    "\n",
    "                list2 = identification.chunk_search(str1, chunked1)\n",
    "                if len(list2) != 0:\n",
    "                    m = list2[len(list2) - 1]\n",
    "\n",
    "                    str4 = nonClause.get_chunk(chunked1[m])\n",
    "                    str4 = identification.verbphrase_identify(str4)\n",
    "                    str5 = \"\"\n",
    "                    str6 = \"\"\n",
    "\n",
    "                    for k in range(m):\n",
    "                        if k in list2:\n",
    "                            str5 += nonClause.get_chunk(chunked1[k])\n",
    "                        else:\n",
    "                            str5 += (chunked1[k][0] + \" \")\n",
    "\n",
    "                    for k in range(m + 1, len(chunked1)):\n",
    "                        if k in list2:\n",
    "                            str6 += nonClause.get_chunk(chunked1[k])\n",
    "                        else:\n",
    "                            str6 += (chunked1[k][0] + \" \")\n",
    "\n",
    "                    st = str5 + str2 + str4 + str6 + str3\n",
    "                    for l in range(num + 1, len(segment_set)):\n",
    "                        st += (\",\" + segment_set[l])\n",
    "                    st += '?'\n",
    "                    st = identification.postprocess(st)\n",
    "                    # st = 'Q.' + st\n",
    "                    list3.append(st)\n",
    "\n",
    "    return list3\n",
    "\n",
    "\n",
    "def howmuch_3(segment_set, num, ner):\n",
    "    tok = nltk.word_tokenize(segment_set[num])\n",
    "    tag = nltk.pos_tag(tok)\n",
    "    gram = r\"\"\"chunk:{<MD>?<VB|VBD|VBG|VBP|VBN|VBZ>+<IN|TO>?<PRP|PRP\\$|NN.?>?<\\$>*<CD>+}\"\"\"\n",
    "    chunkparser = nltk.RegexpParser(gram)\n",
    "    chunked = chunkparser.parse(tag)\n",
    "\n",
    "    list1 = identification.chunk_search(segment_set[num], chunked)\n",
    "    list3 = []\n",
    "\n",
    "    if len(list1) != 0:\n",
    "        for j in range(len(chunked)):\n",
    "            str1 = \"\"\n",
    "            str2 = \"\"\n",
    "            str3 = \"\"\n",
    "            if j in list1:\n",
    "                for k in range(j):\n",
    "                    if k in list1:\n",
    "                        str1 += nonClause.get_chunk(chunked[k])\n",
    "                    else:\n",
    "                        str1 += (chunked[k][0] + \" \")\n",
    "\n",
    "                for k in range(j + 1, len(chunked)):\n",
    "                    if k in list1:\n",
    "                        str3 += nonClause.get_chunk(chunked[k])\n",
    "                    else:\n",
    "                        str3 += (chunked[k][0] + \" \")\n",
    "\n",
    "                strx = nonClause.get_chunk(chunked[j])\n",
    "                tok = nltk.word_tokenize(strx)\n",
    "                tag = nltk.pos_tag(tok)\n",
    "                gram = r\"\"\"chunk:{<MD>?<VB|VBD|VBG|VBP|VBN|VBZ>+<IN|TO>?<PRP|PRP\\$|NN.?>?}\"\"\"\n",
    "                chunkparser = nltk.RegexpParser(gram)\n",
    "                chunked1 = chunkparser.parse(tag)\n",
    "\n",
    "                strx = nonClause.get_chunk(chunked1[0])\n",
    "                str1 += (\" \" + strx)\n",
    "\n",
    "                str2 = ' how much '\n",
    "\n",
    "                tok = nltk.word_tokenize(str1)\n",
    "                tag = nltk.pos_tag(tok)\n",
    "                gram = r\"\"\"chunk:{<EX>?<DT>?<JJ.?>*<NN.?|PRP|PRP\\$|POS|IN|DT|CC|VBG|VBN>+<RB.?>*<VB.?|MD|RP>+}\"\"\"\n",
    "                chunkparser = nltk.RegexpParser(gram)\n",
    "                chunked1 = chunkparser.parse(tag)\n",
    "\n",
    "                list2 = identification.chunk_search(str1, chunked1)\n",
    "\n",
    "                if len(list2) != 0:\n",
    "                    m = list2[len(list2) - 1]\n",
    "\n",
    "                    str4 = nonClause.get_chunk(chunked1[m])\n",
    "                    str4 = identification.verbphrase_identify(str4)\n",
    "                    str5 = \"\"\n",
    "                    str6 = \"\"\n",
    "\n",
    "                    for k in range(m):\n",
    "                        if k in list2:\n",
    "                            str5 += nonClause.get_chunk(chunked1[k])\n",
    "                        else:\n",
    "                            str5 += (chunked1[k][0] + \" \")\n",
    "\n",
    "                    for k in range(m + 1, len(chunked1)):\n",
    "                        if k in list2:\n",
    "                            str6 += nonClause.get_chunk(chunked1[k])\n",
    "                        else:\n",
    "                            str6 += (chunked1[k][0] + \" \")\n",
    "\n",
    "                    st = str5 + str2 + str4 + str6 + str3\n",
    "\n",
    "                    for l in range(num + 1, len(segment_set)):\n",
    "                        st += (\",\" + segment_set[l])\n",
    "                    st += '?'\n",
    "                    st = identification.postprocess(st)\n",
    "                    # st = 'Q.' + st\n",
    "                    list3.append(st)\n",
    "\n",
    "    return list3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
